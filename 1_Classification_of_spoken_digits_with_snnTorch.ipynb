{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panthrrr/ai-in-healthcare/blob/main/1_Classification_of_spoken_digits_with_snnTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284f87f5bee2a601",
      "metadata": {
        "id": "284f87f5bee2a601"
      },
      "source": [
        "# Classification of spoken digits with snnTorch\n",
        "\n",
        "Developed by [Fredrik Sandin](https://www.ltu.se/staff/f/fresan-1.10646?l=en).\n",
        "\n",
        "In this exercise you will learn how to load and transform an event dataset of speech and develop a spiking neural network for classification of spoken digits using snnTorch. See the related tutorials and lecture material referenced in Canvas for background information.\n",
        "\n",
        "There are 3 ***mandatory tasks*** and 1 *optional task* in this exercise.\n",
        "\n",
        "## 1 Import libraries\n",
        "Unless you have already done so, you must first install snnTorch, Tonic, and matplotlib to run this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e981a647718e6c",
      "metadata": {
        "collapsed": true,
        "id": "19e981a647718e6c"
      },
      "outputs": [],
      "source": [
        "!pip install snntorch\n",
        "!pip install tonic\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90fb94bc91573dc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-29T06:54:18.212191Z",
          "start_time": "2023-11-29T06:53:53.314037800Z"
        },
        "id": "d90fb94bc91573dc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "import tonic\n",
        "from tonic import DiskCachedDataset\n",
        "import tonic.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from snntorch import surrogate\n",
        "import snntorch.functional as SF\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f78699b4062d8e",
      "metadata": {
        "id": "83f78699b4062d8e"
      },
      "source": [
        "## 2 Loading event data with Tonic\n",
        "\n",
        "In this exercise you will load an event-based dataset using [Tonic](https://github.com/neuromorphs/tonic), which provides some publicly available event-based vision and audio datasets and event transformations. The package is compatible with snnTorch/PyTorch.\n",
        "\n",
        "This exercise focuses on classifying spoken digits in the [Spiking Heidelberg Digits (SHD)](https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/) dataset, which includes approximately 10000 high-quality aligned studio recordings of spoken digits from 0 to 9 in both German and English language from 12 distinct speakers, out of which two speakers are only present in the test set. Each digit is represented by spikes on 700 channels which were generated using [Lauscher](https://github.com/electronicvisions/lauscher), a neuromorphic [cochlea](https://iopscience.iop.org/article/10.1088/2634-4386/ac4a83#nceac4a83s47) software model. The SHD webpage referenced above includes a leader board and the data set is presented in the following paper.\n",
        "\n",
        "Cramer, B., Stradmann, Y., Schemmel, J., and Zenke, F. (2022).\n",
        "*The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks*.\n",
        "IEEE Transactions on Neural Networks and Learning Systems 33, 2744â€“2757.\n",
        "[https://doi.org/10.1109/TNNLS.2020.3044364](https://doi.org/10.1109/TNNLS.2020.3044364).\n",
        "\n",
        "Recent work by Boeshertz et al. (https://arxiv.org/abs/2407.13534) has show that it can be preferable to use adaptive LIF neurons to generate spikes from the original Heidelberg Digits data, both w.r.t. precision and data reduciton. For those of you who want to are ambiitons and want to expand your knowledge beyond the course content, read this paper, evaluate the proposed neuron model and hypothesize why it generates better spiking data than the alternative we load in this exercise.\n",
        "\n",
        "### 2.1 Load the training dataset and plot samples\n",
        "The SHD training set includes 8156 spoken digit samples. Each digit is represented by an array of events which consists of three parameters (timestamp t, cochlea channel number x, polarity p). Plot the spike representations of five different samples of one particular digit. What are the different channels representing? What is the time duration of the patterns? Why are the patterns different although the same digit is spoken?\n",
        "\n",
        "***Mandatory task 1:***\n",
        "\n",
        "Extend the code below so that it plots one digit sample for each of the 20 classes in the dataset. Thus, the subplot should have 20 panels. Each panel should illustrate one sample of one particular digit. Plot different samples and analyse the variations. Are different samples of one and the same digit similar or different? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ae4e206e21b187",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-29T06:54:28.344276800Z",
          "start_time": "2023-11-29T06:54:28.269921800Z"
        },
        "id": "93ae4e206e21b187"
      },
      "outputs": [],
      "source": [
        "dataset = tonic.datasets.SHD(save_to='./shd', train=True)\n",
        "sensor_size = dataset.sensor_size\n",
        "print(\"Sensor size (# channels):\", sensor_size)\n",
        "print(\"Number of training samples:\", len(dataset))\n",
        "print(\"Event representation: \", dataset.dtype)\n",
        "print(\"Classes:\", [c.decode('UTF-8') for c in dataset.classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ada1844040a765",
      "metadata": {
        "id": "80ada1844040a765"
      },
      "outputs": [],
      "source": [
        "#NEW CODE START\n",
        "samples_for_plotting = {} # Dictionary with pairs\n",
        "classes = [c.decode('UTF-8') for c in dataset.classes]\n",
        "n_classes = len(classes)\n",
        "\n",
        "i = 0 # change to eg. 200 to see other samples of the same classes\n",
        "while len(samples_for_plotting) < n_classes and i < len(dataset):\n",
        "  event, label = dataset[i]\n",
        "  class_name = classes[label]\n",
        "  if class_name not in samples_for_plotting:\n",
        "    samples_for_plotting[class_name] = (event, i)\n",
        "  i += 1\n",
        "print(f\"Collected {len(samples_for_plotting)} class samples\") # Check so 20 samples have been collected\n",
        "\n",
        "ncols = 5\n",
        "nrows = 4\n",
        "\n",
        "fig, axs = plt.subplots(nrows, ncols, figsize=(15, 8), sharey=True)\n",
        "fig.suptitle(\"One sample from each spoken digit class\", fontsize=14)\n",
        "axs = axs.flatten()\n",
        "\n",
        "\n",
        "for class_id, class_name in enumerate(classes):\n",
        "    events, sample_index = samples_for_plotting[class_name]\n",
        "\n",
        "    ax = axs[class_id]\n",
        "    ax.set_visible(True)\n",
        "    ax.scatter(1e-6 * events['t'], events['x'], marker=\".\", s=1)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_title(f\"{class_name}\\n(idx {sample_index})\", fontsize=9)\n",
        "    ax.set_xlabel(\"Time [s]\")\n",
        "    if class_id % ncols == 0: # Put the channel label on every 5 plots\n",
        "        ax.set_ylabel(\"Channel #\")\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95]) # fix spacing so nothing overlaps\n",
        "plt.show()\n",
        "#NEW CODE END\n",
        "\n",
        "samples = 0\n",
        "index = 0\n",
        "digit = \"sechs\"\n",
        "fig, axs = plt.subplots(1,5, sharey=True)\n",
        "fig.suptitle(f\"First five samples of '{digit}' in the training set\")\n",
        "axs[0].set_ylabel(\"Channel #\")\n",
        "while samples < len(axs):\n",
        "    events, label = dataset[index]\n",
        "    if dataset.classes[label].decode('UTF-8') == digit:\n",
        "        axs[samples].scatter(1e-6*events['t'], events['x'], marker=\".\")\n",
        "        axs[samples].invert_yaxis()\n",
        "        axs[samples].set_title(f\"Digit {index}\")\n",
        "        axs[samples].set_xlabel(\"Time [s]\")\n",
        "        print(f\"Digit {index} is represented by {len(events)} events\")\n",
        "        samples += 1\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c01098d6dadd446",
      "metadata": {
        "id": "1c01098d6dadd446"
      },
      "outputs": [],
      "source": [
        "# Tonic provides an event plotting function that can also be used (Time in ms), which rasterizes the events and displays the result with imshow().\n",
        "tonic.utils.plot_event_grid(events)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef4d3138fecaca34",
      "metadata": {
        "id": "ef4d3138fecaca34"
      },
      "source": [
        "### 2.2 Downsample the digits and create a dataloader\n",
        "\n",
        "Now you are familiar with the samples in the dataset. It is time to prepare the digits in a format that is suitable for snnTorch experiments on a PC. To obtain state-of-the-art results we would need to keep a high spatial and temporal resolution of the digits. However, with 700 input channels the simulations would take far too long time if we process the full-resolution digits.\n",
        "\n",
        "We can choose a set of transforms to apply to our data before feeding it to an SNN. Here we [Downsample](https://tonic.readthedocs.io/en/main/autoapi/tonic/transforms/index.html#tonic.transforms.Downsample) the events to 70 channels instead of 700, which reduces the dimension of the resulting tensor representations. Furthermore, we use the [toFrame](https://tonic.readthedocs.io/en/main/generated/tonic.transforms.ToFrame.html#tonic.transforms.ToFrame) transformation with time-binning over 10000 microseconds, which results in a sequence of 10-millisecond long frames with 70 channels each.\n",
        "\n",
        "To speed up dataloading, we can make use of disk caching and batching. That means that once the files are loaded from the original dataset they are written to the disk. Because the event arrays have different lengths, we are going to use a collation function tonic.collation.PadTensors() that will pad shorter recordings to ensure all tensors in a batch have the same dimensions.\n",
        "\n",
        "If you need to refresh basic machine learning concepts such as \"batch\" and \"training set\" a good reference is the [Deep Learning](https://www.deeplearningbook.org/) book, which is part of the course literature in the [D7046E Neural networks and learning machines](https://www.ltu.se/edu/course/D70/D7046E/D7046E-Neuronnat-och-larande-maskiner-1.183276?kursView=kursplan&l=en) course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4112e1e030f37c1",
      "metadata": {
        "id": "b4112e1e030f37c1"
      },
      "outputs": [],
      "source": [
        "batch_size = 16 # Keeping this small for clarity of the examples below, you may need to increase it later\n",
        "\n",
        "# OBS: You must first delete the cache files before running this with new parameters/transforms, otherwise there is no effect of modified hyperparameters\n",
        "\n",
        "trainset = tonic.datasets.SHD(\n",
        "    save_to='./data',\n",
        "    train=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Downsample(sensor_size=sensor_size, spatial_factor=0.1), # 70 channels\n",
        "        transforms.ToFrame(sensor_size=(70,1,1),time_window=10000) # us to 10 ms\n",
        "    ])\n",
        ")\n",
        "\n",
        "cached_trainset = DiskCachedDataset(\n",
        "    trainset,\n",
        "    cache_path='./data/cache/shd/train',\n",
        "    transform=torch.from_numpy\n",
        ")\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    cached_trainset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=tonic.collation.PadTensors()\n",
        ")\n",
        "\n",
        "trainiter = iter(trainloader)\n",
        "\n",
        "# Function that returns the next batch from the training set\n",
        "def load_training_batch():\n",
        "    events, targets = next(trainiter)\n",
        "    # snnTorch torch uses time-first tensors, which means [time x batch_size x feature_dimensions].\n",
        "    # Convert from the default [batch_size x time x 1 x channels] frame representation in the dataset:\n",
        "    events = events.transpose(0,1).squeeze()\n",
        "    return (events, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bde657d5ddb8385",
      "metadata": {
        "id": "1bde657d5ddb8385"
      },
      "source": [
        "### 2.3 Load one batch and visualize the downsampled digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48147d66bf3ac7fc",
      "metadata": {
        "id": "48147d66bf3ac7fc"
      },
      "outputs": [],
      "source": [
        "# Enable plotting of tensors with matplotlib\n",
        "torch.Tensor.ndim = property(lambda self: len(self.shape))\n",
        "\n",
        "events, target = load_training_batch()\n",
        "print(f\"Shape of batch with {batch_size} digits: \", events.shape)\n",
        "\n",
        "fig,axs = plt.subplots(4,4,figsize=(12,12),sharex=True, sharey=True)\n",
        "fig.suptitle(f\"Samples in batch\")\n",
        "\n",
        "for i in range(4):\n",
        "    axs[i,0].set_ylabel('Channel #')\n",
        "    for j in range(4):\n",
        "        index = i*4+j\n",
        "        tensor = events[:,index,:].transpose(0,1)\n",
        "        axs[i,j].imshow(tensor)\n",
        "        axs[i,j].set_title(trainset.classes[target[index]].decode('UTF-8'))\n",
        "        if i==3:\n",
        "            axs[i,j].set_xlabel('Time [10ms frames]')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f822b66f19ea0407",
      "metadata": {
        "id": "f822b66f19ea0407"
      },
      "source": [
        "## 3 Processing of digits with a feed-forward SNN\n",
        "\n",
        "The following is an adaptation of the feed-forward SNN example presented in the snnTorch [tutorial 3](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_3.html). The following code implements a feed-forward SNN that processes the downsampled digits in the SHD trainset visualized above. The code illustrates the principle of how to\n",
        "   * Run a basic SNN simulation with SHD input data\n",
        "   * Visualize the resulting spikes in the hidden and output layer\n",
        "\n",
        "### 3.1 Run SNN simulation over timesteps in the batch\n",
        "\n",
        "***Mandatory task 2:*** How much longer is the simulation time of one forward pass if the original number of channels (700) in the dataset is used? Does the time-binning introduced by the toFrame transformation reduce simulation time? Run simulations to validate your analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdaa1645ef88862f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-29T06:55:41.746818Z",
          "start_time": "2023-11-29T06:55:41.618326800Z"
        },
        "id": "fdaa1645ef88862f"
      },
      "outputs": [],
      "source": [
        "num_timesteps = events.shape[0]     # Number of timesteps in the longest-duration digit\n",
        "num_samples = events.shape[1]       # Number of digits in batch\n",
        "num_channels = events.shape[2]      # Number of channels in downsampled digits\n",
        "num_classes = len(trainset.classes)\n",
        "\n",
        "# SNN hyperparameters\n",
        "beta = 0.95\n",
        "num_inputs = num_channels\n",
        "num_outputs = num_classes\n",
        "hidden_sizefac = 2        # Size of hidden layer in relation to size of input layer\n",
        "\n",
        "# Define first hidden layer and input connections\n",
        "fc1 = nn.Linear(num_inputs, hidden_sizefac*num_channels)\n",
        "lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "# Define output layer with inputs from hidden layer\n",
        "fc2 = nn.Linear(hidden_sizefac*num_channels, num_outputs)\n",
        "lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "# Initialize neuron states\n",
        "mem1 = lif1.init_leaky()\n",
        "mem2 = lif2.init_leaky()\n",
        "\n",
        "# Placeholders for outputs\n",
        "mem2_rec = []\n",
        "spk1_rec = []\n",
        "spk2_rec = []\n",
        "\n",
        "# Run network simulation over all timesteps in the input tensor\n",
        "for step in range(num_timesteps):\n",
        "\n",
        "    cur1 = fc1(events[step,:,:])  # post-synaptic current to hidden <-- spk_in x weight\n",
        "    spk1, mem1 = lif1(cur1, mem1) # mem1[t+1] <--post-syn current contrib + decayed membrane\n",
        "    cur2 = fc2(spk1)              # post-synaptic current to output neurons\n",
        "    spk2, mem2 = lif2(cur2, mem2) # output\n",
        "\n",
        "    # Store states for analysis and plotting\n",
        "    spk1_rec.append(spk1)\n",
        "    spk2_rec.append(spk2)\n",
        "    mem2_rec.append(mem2)\n",
        "\n",
        "# Convert lists to tensors\n",
        "mem2_rec = torch.stack(mem2_rec)\n",
        "spk1_rec = torch.stack(spk1_rec)\n",
        "spk2_rec = torch.stack(spk2_rec)\n",
        "\n",
        "print(\"Hidden spikes shape\", spk1_rec.shape)\n",
        "print(\"Output spikes shape\", spk2_rec.shape)\n",
        "print(\"Output potential shape:\", mem2_rec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW CODE START\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import snntorch as snn\n",
        "events_ds = events\n",
        "\n",
        "def run_forward(events, num_channels, num_classes, hidden_sizefac=2, beta=0.95):\n",
        "    num_timesteps = events.shape[0]     # Number of timesteps in the longest-duration digit\n",
        "    num_samples = events.shape[1]       # Number of digits in batch\n",
        "\n",
        "\n",
        "    # SNN hyperparameters\n",
        "    num_inputs = num_channels\n",
        "    num_outputs = num_classes\n",
        "\n",
        "    # Define first hidden layer and input connections\n",
        "    fc1 = nn.Linear(num_inputs, hidden_sizefac*num_channels)\n",
        "    lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "    # Define output layer with inputs from hidden layer\n",
        "    fc2 = nn.Linear(hidden_sizefac*num_channels, num_outputs)\n",
        "    lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "    # Initialize neuron states\n",
        "    mem1 = lif1.init_leaky()\n",
        "    mem2 = lif2.init_leaky()\n",
        "\n",
        "    # Placeholders for outputs\n",
        "    mem2_rec = []\n",
        "    spk1_rec = []\n",
        "    spk2_rec = []\n",
        "\n",
        "    # Run network simulation over all timesteps in the input tensor\n",
        "    for step in range(num_timesteps):\n",
        "\n",
        "        cur1 = fc1(events[step,:,:])  # post-synaptic current to hidden <-- spk_in x weight\n",
        "        spk1, mem1 = lif1(cur1, mem1) # mem1[t+1] <--post-syn current contrib + decayed membrane\n",
        "        cur2 = fc2(spk1)              # post-synaptic current to output neurons\n",
        "        spk2, mem2 = lif2(cur2, mem2) # output\n",
        "\n",
        "        # Store states for analysis and plotting\n",
        "        spk1_rec.append(spk1)\n",
        "        spk2_rec.append(spk2)\n",
        "        mem2_rec.append(mem2)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    mem2_rec = torch.stack(mem2_rec)\n",
        "    spk1_rec = torch.stack(spk1_rec)\n",
        "    spk2_rec = torch.stack(spk2_rec)\n",
        "\n",
        "    return mem2_rec, spk1_rec, spk2_rec\n",
        "\n",
        "\n",
        "# run the forward pass with downsampled/full 700 channel version of data\n",
        "\n",
        "num_classes = len(trainset.classes)\n",
        "\n",
        "# Downsampled version\n",
        "start = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    run_forward(events_ds, num_channels=events_ds.shape[2], num_classes=num_classes)\n",
        "elapsed_ds = time.perf_counter() - start\n",
        "\n",
        "print(f\"How much longer is the simulation time of one forward pass if the original number of channels (700) in the dataset is used?\")\n",
        "print(f\"Downsampled: {elapsed_ds:.4f} s\")\n",
        "\n",
        "# Full 700-channel version\n",
        "# FULL 700-CHANNEL DATASET (no Downsample)\n",
        "trainset_full = tonic.datasets.SHD(\n",
        "    save_to='./data',\n",
        "    train=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToFrame(sensor_size=(sensor_size[0],1,1), time_window=10000)\n",
        "    ])\n",
        ")\n",
        "\n",
        "cached_trainset_full = DiskCachedDataset(\n",
        "    trainset_full,\n",
        "    cache_path='./data/cache/shd/train_full',  # different cache path\n",
        "    transform=torch.from_numpy\n",
        ")\n",
        "\n",
        "trainloader_full = DataLoader(\n",
        "    cached_trainset_full,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=tonic.collation.PadTensors()\n",
        ")\n",
        "\n",
        "trainiter_full = iter(trainloader_full)\n",
        "\n",
        "def load_training_batch_full():\n",
        "    events_full, targets_full = next(trainiter_full)\n",
        "    events_full = events_full.transpose(0, 1).squeeze()  # [T, B, 700]\n",
        "    return events_full, targets_full\n",
        "\n",
        "events_full, target_full = load_training_batch_full()\n",
        "#print(\"Full batch shape:\", events_full.shape)  # should be [T_full, 16, 700]\n",
        "\n",
        "# Full 700-channel version\n",
        "start = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    run_forward(events_full, num_channels=events_full.shape[2], num_classes=num_classes)\n",
        "elapsed_full = time.perf_counter() - start\n",
        "print(f\"Full 700 ch: {elapsed_full:.4f} s\")\n",
        "\n",
        "print(f\"Speed factor (full / downsampled): {elapsed_full / elapsed_ds:.2f}x\")\n",
        "\n",
        "# 10 ms frames (baseline)\n",
        "trainset_10ms = tonic.datasets.SHD(\n",
        "    save_to='./data',\n",
        "    train=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Downsample(sensor_size=sensor_size[0], spatial_factor=0.1),  # 70 ch\n",
        "        transforms.ToFrame(sensor_size=(70,1,1), time_window=10_000)\n",
        "    ])\n",
        ")\n",
        "\n",
        "# 5 ms frames (more bins in time)\n",
        "trainset_5ms = tonic.datasets.SHD(\n",
        "    save_to='./data',\n",
        "    train=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Downsample(sensor_size=sensor_size, spatial_factor=0.1),  # 70 ch\n",
        "        transforms.ToFrame(sensor_size=(70,1,1), time_window=5_000)\n",
        "    ])\n",
        ")\n",
        "\n",
        "# Cache + loaders for 10 ms\n",
        "cached_trainset_10ms = DiskCachedDataset(\n",
        "    trainset_10ms,\n",
        "    cache_path='./data/cache/shd/train_10ms',\n",
        "    transform=torch.from_numpy\n",
        ")\n",
        "\n",
        "trainloader_10ms = DataLoader(\n",
        "    cached_trainset_10ms,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=tonic.collation.PadTensors()\n",
        ")\n",
        "\n",
        "# Cache + loaders for 5 ms\n",
        "cached_trainset_5ms = DiskCachedDataset(\n",
        "    trainset_5ms,\n",
        "    cache_path='./data/cache/shd/train_5ms',\n",
        "    transform=torch.from_numpy\n",
        ")\n",
        "\n",
        "trainloader_5ms = DataLoader(\n",
        "    cached_trainset_5ms,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=tonic.collation.PadTensors()\n",
        ")\n",
        "\n",
        "print(\"--------------------\")\n",
        "# Get one batch from each\n",
        "events_10ms, target_10ms = next(iter(trainloader_10ms))\n",
        "events_10ms = events_10ms.transpose(0, 1).squeeze()  # [T_10, B, 70]\n",
        "\n",
        "events_5ms, target_5ms = next(iter(trainloader_5ms))\n",
        "events_5ms = events_5ms.transpose(0, 1).squeeze()    # [T_5, B, 70]\n",
        "\n",
        "\n",
        "print(\"Does the time-binning introduced by the toFrame transformation reduce simulation time? \")\n",
        "#print(\"10 ms batch shape:\", events_10ms.shape)\n",
        "#print(\" 5 ms batch shape:\", events_5ms.shape)\n",
        "\n",
        "# Time forward passes\n",
        "start = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    run_forward(events_10ms, num_channels=events_10ms.shape[2], num_classes=num_classes)\n",
        "t_10 = time.perf_counter() - start\n",
        "print(f\"10 ms frames time: {t_10:.4f} s\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "with torch.no_grad():\n",
        "    run_forward(events_5ms, num_channels=events_5ms.shape[2], num_classes=num_classes)\n",
        "t_5 = time.perf_counter() - start\n",
        "print(f\" 5 ms frames time: {t_5:.4f} s\")\n",
        "\n",
        "print(f\"Time ratio (5 ms / 10 ms): {t_5 / t_10:.2f}x\")\n",
        "\n",
        "print(\"------------------\")\n",
        "\n",
        "print(\"Time binning reduces simulation time because having fewer bins means that more spikes are being put into one bin, and there will be less simulations.\")\n",
        "\n",
        "#NEW CODE END"
      ],
      "metadata": {
        "id": "fhXuA-Tgyhsr"
      },
      "id": "fhXuA-Tgyhsr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761edc794038f7eb",
      "metadata": {
        "id": "761edc794038f7eb"
      },
      "outputs": [],
      "source": [
        "# Plot activity of hidden neurons for each input digit in the batch\n",
        "s = spk1_rec.detach().numpy()\n",
        "fig,axs = plt.subplots(4,4,figsize=(8,12),sharex=True,sharey=True)\n",
        "fig.suptitle(f\"Hidden neuron spikes\")\n",
        "fig.tight_layout()\n",
        "for i in range(4):\n",
        "    axs[i,0].set_ylabel('Hidden neuron #')\n",
        "    for j in range(4):\n",
        "        index = i*4+j\n",
        "        tensor = s[:,index,:].squeeze().transpose(1,0)\n",
        "        axs[i,j].imshow(tensor)\n",
        "        axs[i,j].set_title(trainset.classes[target[index]].decode('UTF-8'))\n",
        "        if i==3:\n",
        "            axs[i,j].set_xlabel('Time [10ms frames]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "175035fa83d0fa66",
      "metadata": {
        "id": "175035fa83d0fa66"
      },
      "outputs": [],
      "source": [
        "# Plot activity of output neurons for each input digit in the batch\n",
        "s = spk2_rec.detach().numpy()\n",
        "fig,axs = plt.subplots(4,4,figsize=(8,4),sharex=True,sharey=True)\n",
        "fig.suptitle(f\"Output neuron spikes\")\n",
        "fig.tight_layout()\n",
        "for i in range(4):\n",
        "    axs[i,0].set_ylabel('Output #')\n",
        "    for j in range(4):\n",
        "        index = i*4+j\n",
        "        tensor = s[:,index,:].squeeze().transpose(1,0)\n",
        "        axs[i,j].imshow(tensor)\n",
        "        axs[i,j].set_title(trainset.classes[target[index]].decode('UTF-8'))\n",
        "        if i==3:\n",
        "            axs[i,j].set_xlabel('Time [10ms frames]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fff300dcef417cb",
      "metadata": {
        "id": "2fff300dcef417cb"
      },
      "source": [
        "## 4 Train an SNN to classify the digits\n",
        "\n",
        "***Mandatory task 3:***\n",
        "\n",
        "Based on your understanding of the code above and the [snnTorch tutorial 5](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_5.html) (and any other documented functions in snnTorch) that you think are required, develop and train a feed-forward SNN for classification of the 20 different digits in the SHD dataset.\n",
        "\n",
        "The goal is that after training the 20 output neurons of the networks should differentiate between the 20 different digits in the training set, in the sense that for each target/class one particular output neuron is more likely to fire and/or fires with a higher spikerate than the other 19 neurons.\n",
        "\n",
        "Note that you cannot directly copy the example in tutorial 5 because the SHD digits have varying duration in time (while the MNIST samples in the tutorial have fixed dimensionality and consequently the inputs could be flattened to vectors in that case). Here you need to consider and optimize the response of the output neurons over time when each digit is presented to the network.\n",
        "\n",
        "Plot the ***loss curve*** from the start until the end of training (over all \"epochs\"). The goal is that the loss should decrease and eventually converge or fluctuate at a low value. What is the ***accuracy of your classifier*** before and after training? Plot a ***confusion matrix***, what are the weaknesses/strengths of your classifier based on the result?\n",
        "\n",
        "***Hints:***\n",
        "\n",
        "Use the discussion forum for exercises in Canvas if you get stuck. Share what you have learned and what question you have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2ddc4d24888bf5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-29T06:37:31.634990500Z",
          "start_time": "2023-11-29T06:37:31.626481Z"
        },
        "id": "6c2ddc4d24888bf5"
      },
      "outputs": [],
      "source": [
        "# Develop and train SNN classifier\n",
        "# NEW CODE START\n",
        "\n",
        "# Builds a feed-forward SNN and trains it to classify the 20 spoken digits in the dataset.\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Hyperparameters and settings\n",
        "# -----------------------------\n",
        "\n",
        "beta = 0.95 # membrane decay constant (leaky integrate-and-fire)\n",
        "hidden_sizefac = 2\n",
        "num_hidden = hidden_sizefac * num_channels # hidden layer size (num_channels from earlier code)\n",
        "\n",
        "num_epochs = 10  # Number of training passes\n",
        "                # 5 was slow, change to more epochs for higher accuracy\n",
        "\n",
        "max_batches_per_epoch = None # LIMIT TRAINING BATCHES PER EPOCH for speed\n",
        "                            # (None = use all) lower value => faster but less trained\n",
        "\n",
        "# Use GPU if available in Colab, otherwise cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# SNN model definition\n",
        "# -----------------------------\n",
        "\n",
        "# inspired by the snnTorch tutorial 5 and the demo code in the notebook.\n",
        "# Here we adapt it to temporal SHD data [T, B, C] instead of static MNIST images.\n",
        "\n",
        "\n",
        "class SHDNet(nn.Module): # Class for our SHD network\n",
        "\n",
        "# Input (C channels) -> Hidden LIF layer -> Output LIF layer (20 classes).\n",
        "# Time: looping over T timesteps in forward().\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, beta=0.95):\n",
        "        super().__init__()\n",
        "\n",
        "        # Fully connected layer from input channels to hidden neurons\n",
        "        self.fc1  = nn.Linear(num_inputs, num_hidden)\n",
        "\n",
        "        # Leaky Integrate-and-Fire layer with surrogate gradient for backprop\n",
        "        self.lif1 = snn.Leaky(beta=beta,\n",
        "                              spike_grad=surrogate.fast_sigmoid(slope=25))# surrogate gradient\n",
        "\n",
        "        # Fully connected layer from hidden to output neurons (one per class)\n",
        "        self.fc2  = nn.Linear(num_hidden, num_outputs)\n",
        "\n",
        "        # Second LIF layer at the output, also with surrogate gradient\n",
        "        self.lif2 = snn.Leaky(beta=beta,\n",
        "                              spike_grad=surrogate.fast_sigmoid(slope=25))# surrogate gradient\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        x: Input spike frames with shape [T, B, C]\n",
        "           T = number of time steps (frames)\n",
        "           B = batch size (number of digits in the batch)\n",
        "           C = number of channels (70 after downsampling)\n",
        "        Returns:\n",
        "            spk2_rec: output spikes over time, shape [T, B, num_outputs]\n",
        "            mem2_rec: output membrane potentials over time, same shape\n",
        "        \"\"\"\n",
        "\n",
        "        T, B, _ = x.shape\n",
        "\n",
        "        # Initialize membrane potentials for both LIF layers.\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "\n",
        "        # Lists to record output spikes and membrane potentials at each time step\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        # Loop over time\n",
        "        for t in range(T):\n",
        "            # x[t] has shape [B, C]. This is all the samples at time t\n",
        "\n",
        "            cur1 = self.fc1(x[t])           # Linear current into hidden layer [B, num_hidden]\n",
        "            spk1, mem1 = self.lif1(cur1, mem1) # Hidden spikes + updated membrane\n",
        "\n",
        "            cur2 = self.fc2(spk1)           # Linear current into output layer [B, num_outputs]\n",
        "            spk2, mem2 = self.lif2(cur2, mem2) # Output spikes + updated membrane\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        # Stack the lists into tensors with time as first dimension\n",
        "        spk2_rec = torch.stack(spk2_rec, dim=0)   # [T, B, num_outputs]\n",
        "        mem2_rec = torch.stack(mem2_rec, dim=0)   # [T, B, num_outputs]\n",
        "        return spk2_rec, mem2_rec\n",
        "\n",
        "# Instantiate the network with our chosen sizes and move it to CPU/GPU\n",
        "net = SHDNet(num_inputs=num_channels,\n",
        "             num_hidden=num_hidden,\n",
        "             num_outputs=num_classes,\n",
        "             beta=beta).to(device)\n",
        "print(net)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Loss function and optimizer\n",
        "# -----------------------------\n",
        "\n",
        "# as in Tutorial 5.\n",
        "\n",
        "loss_fn   = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
        "\n",
        "def batch_accuracy(spk_rec, targets):\n",
        "    # rate code: sum spikes over time to get a spike count per neuron\n",
        "    spk_sum = spk_rec.sum(dim=0)         # [B, num_outputs]\n",
        "    _, pred = spk_sum.max(1)             # index of neuron with most spikes\n",
        "    correct = (pred == targets).float().mean().item()\n",
        "    return correct\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Test set + loader\n",
        "# -----------------------------\n",
        "\n",
        "# Like previously in this notebook\n",
        "\n",
        "testset = tonic.datasets.SHD(\n",
        "    save_to='./data',\n",
        "    train=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Downsample(sensor_size=sensor_size[0], spatial_factor=0.1),\n",
        "        transforms.ToFrame(sensor_size=(70,1,1), time_window=10_000)\n",
        "    ])\n",
        ")\n",
        "\n",
        "cached_testset = DiskCachedDataset(\n",
        "    testset,\n",
        "    cache_path='./data/cache/shd/test',\n",
        "    transform=torch.from_numpy\n",
        ")\n",
        "\n",
        "testloader = DataLoader(\n",
        "    cached_testset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=tonic.collation.PadTensors()\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Helper functions\n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "# Convert a batch from the DataLoader into the format expected by the SNN.\n",
        "# Input events: [B, T, 1, C]  (batch, time, dummy, channels)\n",
        "# Output events: [T, B, C]\n",
        "def prep_batch(batch):\n",
        "    events, targets = batch               # events: [B, T, 1, C]\n",
        "    events = events.transpose(0, 1).squeeze()  # [B, T, 1, C] -> [T, B, C]\n",
        "    events = events.to(device).float() # move to cpu/gpu\n",
        "    targets = targets.to(device).long()\n",
        "    return events, targets\n",
        "\n",
        "# Check accuracy on ONE batch from a loader\n",
        "def eval_one_batch(loader, net):\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(loader)) # Get one batch\n",
        "        events, targets = prep_batch(batch) # Format batch to fit [T, B, C]\n",
        "        spk_rec, mem_rec = net(events)\n",
        "        acc = batch_accuracy(spk_rec, targets)\n",
        "    return acc\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Accuracy BEFORE training\n",
        "# -----------------------------\n",
        "\n",
        "acc_train_before = eval_one_batch(trainloader, net)\n",
        "acc_test_before  = eval_one_batch(testloader, net)\n",
        "\n",
        "print(f\"Train batch acc BEFORE training: {acc_train_before*100:.2f}%\")\n",
        "print(f\"Test  batch acc BEFORE training: {acc_test_before*100:.2f}%\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop\n",
        "# -----------------------------\n",
        "# Loops over time since the inputs are sequences\n",
        "\n",
        "train_loss_hist = [] # List to store average train loss per epoch\n",
        "test_loss_hist  = [] # List to store test loss on a single batch per epoch\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train() # set SNN model to training mode\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    # Iterate over batches from the training loader\n",
        "    for n_batches, batch in enumerate(trainloader):\n",
        "\n",
        "        # Limit the number of batches in each epoch, stop if our limit is reached\n",
        "        if max_batches_per_epoch is not None and n_batches >= max_batches_per_epoch:\n",
        "          break\n",
        "\n",
        "        events, targets = prep_batch(batch) # Format batch to fit [T, B, C]\n",
        "\n",
        "        # -------- Forward pass --------\n",
        "        spk_rec, mem_rec = net(events)      # mem_rec: [T,B,num_classes]\n",
        "\n",
        "        # -------- Loss computation --------\n",
        "        # loss summed over all timesteps\n",
        "        # Encourages the correct output neuron to be active during the whole duration of that input digit.\n",
        "        loss_val = 0.0\n",
        "        for t in range(mem_rec.size(0)):\n",
        "            loss_val = loss_val + loss_fn(mem_rec[t], targets)\n",
        "\n",
        "        # -------- Backward pass and update --------\n",
        "        optimizer.zero_grad() # reset gradients\n",
        "        loss_val.backward() # compute gradients (using surrogate spikes)\n",
        "        optimizer.step() # update weights\n",
        "\n",
        "\n",
        "        running_loss += loss_val.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    # Average training loss over the batches we used\n",
        "    avg_train_loss = running_loss / n_batches\n",
        "    train_loss_hist.append(avg_train_loss)\n",
        "\n",
        "    # test loss on one batch for monitoring\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        test_batch = next(iter(testloader))\n",
        "        events_t, targets_t = prep_batch(test_batch)\n",
        "        spk_rec_t, mem_rec_t = net(events_t)\n",
        "        test_loss_val = 0.0\n",
        "        for t in range(mem_rec_t.size(0)):\n",
        "            test_loss_val = test_loss_val + loss_fn(mem_rec_t[t], targets_t)\n",
        "        test_loss_hist.append(test_loss_val.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "          f\"train loss={avg_train_loss:.3f}, test loss={test_loss_val.item():.3f}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Plotting the loss function\n",
        "# -----------------------------\n",
        "# Shows how the loss changes over epochs.\n",
        "# We expect it to go down and then level out.\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "epochs = np.arange(1, num_epochs + 1)\n",
        "plt.plot(epochs, train_loss_hist, marker='o', label=\"Train loss\")\n",
        "plt.plot(epochs, test_loss_hist, marker='o', label=\"Test loss (1 batch)\")\n",
        "plt.xticks(epochs)  # force x axis marks to be 1, 2, 3, ... to count epochs\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend()\n",
        "plt.title(\"SNN training on SHD\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Accuracy AFTER training\n",
        "# -----------------------------\n",
        "# Evaluating the trained model on the full train and test sets.\n",
        "\n",
        "\n",
        "def evaluate_loader(loader, net):\n",
        "    net.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            events, targets = prep_batch(batch)\n",
        "            spk_rec, mem_rec = net(events)\n",
        "            spk_sum = spk_rec.sum(dim=0)      # [B, num_classes]\n",
        "            _, pred = spk_sum.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += (pred == targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "acc_train_after = evaluate_loader(trainloader, net)\n",
        "acc_test_after  = evaluate_loader(testloader, net)\n",
        "\n",
        "print(f\"Train accuracy AFTER training: {acc_train_after*100:.2f}%\")\n",
        "print(f\"Test  accuracy AFTER training: {acc_test_after*100:.2f}%\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Confusion matrix on test set\n",
        "# -----------------------------\n",
        "\n",
        "all_targets = []\n",
        "all_preds   = []\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in testloader:\n",
        "        events, targets = prep_batch(batch)\n",
        "        spk_rec, mem_rec = net(events)\n",
        "        spk_sum = spk_rec.sum(dim=0)      # [B, num_classes]\n",
        "        _, pred = spk_sum.max(1)\n",
        "\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "        all_preds.append(pred.cpu().numpy())\n",
        "\n",
        "# Concatenate predictions from all batches\n",
        "all_targets = np.concatenate(all_targets)\n",
        "all_preds   = np.concatenate(all_preds)\n",
        "\n",
        "# Build confusion matrix: rows = true class, columns = predicted class\n",
        "cm = confusion_matrix(all_targets, all_preds, labels=range(num_classes))\n",
        "print(\"Confusion matrix shape:\", cm.shape)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title(\"Confusion matrix (SHD, SNN)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(num_classes)\n",
        "class_names = [c.decode('UTF-8') for c in trainset.classes]\n",
        "plt.xticks(tick_marks, class_names, rotation=90)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# NEW CODE END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41c52f4d368500e",
      "metadata": {
        "id": "e41c52f4d368500e"
      },
      "source": [
        "## 5 Test and improve the trained SNN\n",
        "\n",
        "***Optional task:***\n",
        "\n",
        "Test and improve the accuracy of your SNN classifier. How high accuracy can you achieve? How close to the SHD leader board is your result? Can you benefit from spiking CNNs, LSTMSs, ...?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313afd0022ca24b0",
      "metadata": {
        "id": "313afd0022ca24b0"
      },
      "outputs": [],
      "source": [
        "testset = tonic.datasets.SHD(save_to='./data', train=False)\n",
        "\n",
        "cached_testset = DiskCachedDataset(testset, transform=...\n",
        "\n",
        "testloader = DataLoader(cached_testset, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "705c2520c245f08d",
      "metadata": {
        "id": "705c2520c245f08d"
      },
      "source": [
        "## References\n",
        "\n",
        "The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks;\n",
        "[https://doi.org/10.1109/TNNLS.2020.3044364](https://doi.org/10.1109/TNNLS.2020.3044364).\n",
        "\n",
        "Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks; [https://doi.org/10.1109/MSP.2019.2931595](https://doi.org/10.1109/MSP.2019.2931595).\n",
        "\n",
        "Training Spiking Neural Networks Using Lessons From Deep Learning; [https://doi.org/10.1109/JPROC.2023.3308088](https://doi.org/10.1109/JPROC.2023.3308088)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}